---
title: "Univariate Stochastic Volatility Model with Texting and Not Texting"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cahce = TRUE)

library(here)
library(rstan)
library(dplyr)
library(ggplot2)
library(digest)
library(purrr)
library(aws.s3)
library(aws.ec2metadata)
library(aws.signature)

util <- new.env()
source(here('src', 'codebase', 'stan_utility.R'), local=util)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

load(here('md_lane.RData'))
input_data <- list("y" = md_half$Lane.Position - mean(md_half$Lane.Position),
                   "texting" = as.integer(md_half$Stimulus),
                   "N" = length(md_half$Lane.Position),
                   "N_texting" = length(unique(as.integer(md_half$Stimulus))))
colors <- list(c_light=c("#DCBCBC"), c_light_highlight=c("#C79999"),
               c_mid=c("#B97C7C"),   c_mid_highlight=c("#A25050"),
               c_dark=c("#8F2727"),  c_dark_highlight=c("#7C0000"),
               d_light=c("#BCBCDC"), d_light_highlight=c("#9999C7"),
               d_mid=c("#7C7CB9"),   d_mid_highlight=c("#5050A2"),
               d_dark=c("#27278F"),  d_dark_highlight=c("#00007c"))
```

```{r data_plot}
ggplot(md_half, aes(Distance, Lane.Position)) +
    geom_line() +
    geom_point(aes(colour=Stimulus), size=1.5) +
    theme_bw() + 
    theme(legend.position = "bottom",
          legend.background = element_rect(fill="gray90", size=.5, linetype="dotted")) + 
    ggtitle("Texting Driving") +
    xlab("Distance (m)") + 
    ylab("Lane Position(m)") +
    coord_cartesian(ylim=c(1.5, 2.5)) +
    scale_color_manual(labels=c("No Distraction", "Texting"),
                       values=c(colors$c_mid_highlight, colors$d_mid_highlight))
```

# 1. Conceptual Analysis
AR process is intended to have exponential decay but there exists some ringing of behavior in the later lags, a more flexible model architecture is needed to account for this behavior. The stochastic volatility model has more parameters to fit the lags that exhibit the longer frequency trend. There are two states of volatility of interest: mean volatility during normal driving and texting driving.

# 2. Define observations
```{r obs}
writeLines(readLines(here("src", "stan", "sv2.stan"), n=6))
```

# 3. Summary statistics
Correlogram of variances, histogram of variance

# 4. Build generative model
```{r generative_model}
writeLines(readLines(here("src", "stan", "gen_sv2.stan")))
```

```{r model}
writeLines(readLines(here("src", "stan", "sv2.stan")))
```

# 5. Analyze generative ensemble

```{r generate_ensemble, results="hide"}
R <- 1000;
simu_data <- list("N" = input_data$N,
                  "texting" = input_data$texting,
                  "N_texting" = input_data$N_texting)
fit <- stan(here("src", "stan", "gen_sv2.stan"), 
            data=simu_data, iter=R, warmup=0, chains=1, refresh=R, seed=234987,
            algorithm="Fixed_param")
simu_mus <- extract(fit)$mu
simu_phis <- extract(fit)$phi
simu_sigmas <- extract(fit)$sigma
simu_deltas <- extract(fit)$delta_texting
simu_hs <- extract(fit)$h
simu_ys <- extract(fit)$y
hash <- sha1(c(readLines(here("src", "stan", "gen_sv2.stan")),
               readLines(here("src", "stan", "sv2.stan")), 
               as.character(R)))
```

```{r prior_predictive}
par(cex=0.8)
probs <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)
# prior predictive time series
post <- as.matrix(fit)
sel <- grep("y", colnames(post))
ci50 <- matrix(NA, nrow = length(sel), ncol = 2)
ci90 <- matrix(NA, nrow = length(sel), ncol = 2)
for (i in 1:length(sel)) {
  ci50[i,] <- quantile(post[,sel[i]], prob = c(0.25, 0.75), names = FALSE)
  ci90[i,] <- quantile(post[,sel[i]], prob = c(0.05, 0.95), names = FALSE)
}

plot(0, type= "n", 
     xlim = c(0,length(input_data$y)), ylim = c(-2, 2),
     xlab = "Distance", ylab = "Lane Position (mean corrected)",
     main = "Prior Predictive Lane Positioning\n50% and 90% intervals")
t <- 1:input_data$N
polygon(c(rev(t), t), c(rev(ci90[,1]), ci90[,2]), col = colors$c_light, border = FALSE)
polygon(c(rev(t), t), c(rev(ci50[,1]), ci50[,2]), col = colors$c_mid, border = FALSE)
t <- which(input_data$texting==2)
polygon(c(rev(t), t), c(rev(ci90[t,1]), ci90[t,2]), col = colors$d_light, border = FALSE)
polygon(c(rev(t), t), c(rev(ci50[t,1]), ci50[t,2]), col = colors$d_mid, border = FALSE)
legend("bottomright", c("No stimulus", "Texting"),
       col = c(colors$c_mid, colors$d_mid),
       lwd = c(2,2,2), text.font=0.5)
```

```{r create_zips, echo=FALSE, results="hide"}
simu_list <- t(data.matrix(data.frame(simu_mus, simu_phis, simu_sigmas, simu_deltas, simu_ys)))
stan_file <- 'sv2.stan'
fit_model <- stan_model(here('src', 'stan', stan_file))
dir.create(paste("/tmp/zip_files/", hash, "/", sep=""))

create_zip <- function(r, hash, simu_list, stan_file) {
  library(here)
  simu <- simu_list[, r]
  simu_mu <- simu[1]
  simu_phi <- simu[2]
  simu_sigma <- simu[3]
  simu_delta <- simu[4]
  simu_y <- simu[5:length(simu_list[, 1])]
  
  stan_name <- strsplit(stan_file, "[.]")[[1]][1]
  tmp_dir <- paste("/tmp/", hash, "_", as.character(r), sep="")
  zip_file_path <- paste("/tmp/zip_files/", hash, "/", 
                         hash, "_", as.character(r), ".zip", sep="")
  simu_data_path <- paste(tmp_dir, "/simu_data.Rdata", sep="")
  myjob_path <- here("src", "docker", "myjob.sh")
  rds_path <- here("src", "stan",  paste(
    strsplit(stan_file, "[.]")[[1]][1], ".rds", sep=""))
  stan_path <- here("src", "stan", stan_file)
  r_script_path <- here("src", "docker", 
                        stan_name, paste(
                          strsplit(stan_file, "[.]")[[1]][1], ".R", sep=""))
  s3_key <- paste("stan_runs/", stan_file, "/", hash, "_", as.character(r), ".zip", sep="")
  
  dir.create(tmp_dir)
  simu_data <- list("y" = simu_y, 
                   "N" = length(simu_y), 
                   "N_texting" = input_data$N_texting, 
                   "texting" = input_data$texting)
  file.copy(from=here('src', 'codebase', 'stan_utility.R'), to=tmp_dir)
  file.copy(from=stan_path, to=tmp_dir)
  file.copy(from=rds_path, to=tmp_dir)
  file.copy(from=myjob_path, to=tmp_dir)
  file.copy(from=r_script_path, to=tmp_dir)
  save(simu_data, simu_mu, simu_sigma, simu_phi, simu_delta, file=simu_data_path)  
  files2zip <- dir(tmp_dir, full.names = TRUE)
  zip(zipfile = zip_file_path, files = files2zip)
  aws.s3::put_object(zip_file_path, object=s3_key, bucket="stat685-batch") 
}
```

```{r send_to_aws, results="hide"}
1 %>% map(function(x) create_zip(x, hash, simu_list, "sv2.stan"))
library(foreach)
library(doParallel)
registerDoParallel(makeCluster(detectCores()))
foreach(i=2:R) %dopar% create_zip(i, hash, simu_list, stan_file)
```

```{r cleanup_prep_results}
system(paste("rm -rf ", "/tmp/", hash, "*", sep=""))
system(paste("rm -rf ", "/tmp/zip_files/", hash, "/", sep=""))

result_dir <- here("etc", "results", stan_file)
dir.create(result_dir)
system(paste("aws s3 cp s3://stat685-batch/results/", stan_file, "/ ", result_dir, "/ --recursive", sep=""))
```

**Fit the simulated observations and evaluate
```{r}
output_list <- dir(result_dir)
ensemble_output <- foreach(output=output_list, .combine='cbind') %dopar% {
   readRDS(paste(result_dir, "/", output, sep=""))
}
```

```{r warning_codes}
warning_code <- ensemble_output[1,]
if (sum(warning_code) != 0) {
  percentage <- length(warning_code[warning_code!=0]) / length(output_list) * 100
  print ("Some simulated posterior fits in the generative ensemble encountered problems!")
  print ("Percentage with problems:")
  print (percentage)
} else {
  print ("No posterior fits in the generative ensemble encountered problems!")
}
```

```{r sbc}
png(filename=here("src", "presentation", "graphics", "prior", "sv2_sbc_shrink.png"),
    width=800, height=600)
par(mfrow=c(2, 4))
sbc_rank <- ensemble_output[2,]
sbc_hist <- hist(sbc_rank, seq(0, 500, 25) - 0.5, plot=FALSE)
plot(sbc_hist, main="", xlab="Prior Rank (mu)", yaxt='n', ylab="")
low <- qbinom(0.005, R, 1 / 20)
mid <- qbinom(0.5, R, 1 / 20)
high <- qbinom(0.995, R, 1 / 20)
bar_x <- c(-10, 510, 500, 510, -10, 0, -10)
bar_y <- c(high, high, mid, low, low, mid, high)
polygon(bar_x, bar_y, col=c("#DDDDDD"), border=NA)
segments(x0=0, x1=500, y0=mid, y1=mid, col=c("#999999"), lwd=2)
plot(sbc_hist, col=colors$c_dark, border=colors$c_dark_highlight, add=T)

sbc_rank <- ensemble_output[5,]
sbc_hist <- hist(sbc_rank, seq(0, 500, 25) - 0.5, plot=FALSE)
plot(sbc_hist, main="", xlab="Prior Rank (phi)", yaxt='n', ylab="")
low <- qbinom(0.005, R, 1 / 20)
mid <- qbinom(0.5, R, 1 / 20)
high <- qbinom(0.995, R, 1 / 20)
bar_x <- c(-10, 510, 500, 510, -10, 0, -10)
bar_y <- c(high, high, mid, low, low, mid, high)
polygon(bar_x, bar_y, col=c("#DDDDDD"), border=NA)
segments(x0=0, x1=500, y0=mid, y1=mid, col=c("#999999"), lwd=2)
plot(sbc_hist, col=colors$c_dark, border=colors$c_dark_highlight, add=T)

sbc_rank <- ensemble_output[8,]
sbc_hist <- hist(sbc_rank, seq(0, 500, 25) - 0.5, plot=FALSE)
plot(sbc_hist, main="", xlab="Prior Rank (sigma)", yaxt='n', ylab="")
low <- qbinom(0.005, R, 1 / 20)
mid <- qbinom(0.5, R, 1 / 20)
high <- qbinom(0.995, R, 1 / 20)
bar_x <- c(-10, 510, 500, 510, -10, 0, -10)
bar_y <- c(high, high, mid, low, low, mid, high)
polygon(bar_x, bar_y, col=c("#DDDDDD"), border=NA)
segments(x0=0, x1=500, y0=mid, y1=mid, col=c("#999999"), lwd=2)
plot(sbc_hist, col=colors$c_dark, border=colors$c_dark_highlight, add=T)

sbc_rank <- ensemble_output[11,]
sbc_hist <- hist(sbc_rank, seq(0, 500, 25) - 0.5, plot=FALSE)
plot(sbc_hist, main="", xlab="Prior Rank (delta)", yaxt='n', ylab="")
low <- qbinom(0.005, R, 1 / 20)
mid <- qbinom(0.5, R, 1 / 20)
high <- qbinom(0.995, R, 1 / 20)
bar_x <- c(-10, 510, 500, 510, -10, 0, -10)
bar_y <- c(high, high, mid, low, low, mid, high)
polygon(bar_x, bar_y, col=c("#DDDDDD"), border=NA)
segments(x0=0, x1=500, y0=mid, y1=mid, col=c("#999999"), lwd=2)
plot(sbc_hist, col=colors$c_dark, border=colors$c_dark_highlight, add=T)

# mu
z_score <- ensemble_output[3,]
shrinkage <- ensemble_output[4,]
plot(shrinkage, z_score, col=c("#8F272720"), lwd=2, pch=16, cex=0.8, main="Mu",
     xlim=c(0, 1), xlab="Posterior Shrinkage", ylim=c(0, 5), ylab="Posterior z-Score")
# phi
z_score <- ensemble_output[6,]
shrinkage <- ensemble_output[7,]
plot(shrinkage, z_score, col=c("#8F272720"), lwd=2, pch=16, cex=0.8, main="Phi",
     xlim=c(0, 1), xlab="Posterior Shrinkage", ylim=c(0, 5), ylab="Posterior z-Score")
# sigma
z_score <- ensemble_output[9,]
shrinkage <- ensemble_output[10,]
plot(shrinkage, z_score, col=c("#8F272720"), lwd=2, pch=16, cex=0.8, main="Sigma",
     xlim=c(0, 1), xlab="Posterior Shrinkage", ylim=c(0, 5), ylab="Posterior z-Score")
# delta
z_score <- ensemble_output[12,]
shrinkage <- ensemble_output[13,]
plot(shrinkage, z_score, col=c("#8F272720"), lwd=2, pch=16, cex=0.8, main="Delta",
     xlim=c(0, 1), xlab="Posterior Shrinkage", ylim=c(0, 5), ylab="Posterior z-Score")
par(mfrow = c(1, 1))
dev.off()
```

# 6. Fit observations and evaluate
```{r actual_fit}
fit <- stan(file=here('src', 'stan', 'sv2.stan'), data=input_data, seed=4938483, refresh=2000)
pairs(fit, pars=c("mu", "sigma", "phi", "delta_texting"))
plot(fit, pars=c("mu", "sigma", "phi", "delta_texting"))
```

# 7. Analyze posterior predictive distribution
